---
title: vLLM
date: 2025-01-28
categories: [Tech, Frameworks]
tags: [bert, machine learning, NLP]     # TAG names should always be lowercase
math: true
---

**vLLM** is an optimized, high-performance library for **efficient and scalable serving** of large language models (LLMs). It is designed to maximize throughput and minimize latency for inference, leveraging advanced memory management and efficient scheduling.

Key features:

- **PagedAttention**: A novel memory management technique that enables efficient handling of large sequence lengths.
- **Fast inference**: Optimized CUDA kernels and parallel processing.
- **Easy deployment**: Supports models like LLaMA, Falcon, Mistral, and more with a simple API.
- **Efficient batching**: Dynamic batching for higher throughput.
- **Integration**: Works with Hugging Face Transformers, OpenAI API, and DeepSpeed.



### Compared to Ollama:

![vllm-ollama](/assets/images/vllm-ollama.png)

Choose **vLLM** if you need **fast GPU-based LLM inference, API serving, or multi-GPU scaling**.

Choose **Ollama** if you want a **quick, easy-to-use CLI tool for running LLMs locally (CPU-friendly)**.



### Installation

```shell
pip install vllm
```



> Notice: If encountered "No module named 'grpc'" error, install `grpcio` dependency through pip:
>
> `pip install grpcio grpcio-tools`. 
>
{:  .prompt-tip }



### Run A Model


> Danger: Be careful with the model size and memory. vLLM **reserves** a large portion of GPU memory **in advance** to improve inference speed.
>
> ```
> 显存占用 ≈ max_num_seq * max_model_len * 模型大小（参数量） * 计算精度
> ```
>
{: .prompt-danger}



```python
from vllm import LLM, SamplingParams

# Check GPU Usage: This shows GPU memory consumption, active processes, and utilization.
!nvidia-smi

# Set a/multiple GPU(s) to run
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"  # Use only GPU 0
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"  # Use GPU 0 and GPU 1


# Load the DeepSeek model
llm_model_path = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
llm = LLM(
  model=llm_model_path,
  dtype="half",
  max_num_seqs=1,	# Max # of seqs processed simultaneously
  max_num_batched_tokens=2048,	# Max # of tokens for one (inferrence time)
  max_model_len=2048,	# Max # of tokens per seq
  tensor_parallel_size=4,	# Distribute model on # of GPUs
  gpu_memory_utilization=0.85
)

# Define sampling parameters
sampling_params = SamplingParams(
  temperature=0.7,
  max_tokens=100
)

# Generate text
outputs = llm.generate("What is DeepSeek LLM?", sampling_params)

# Print result
for output in outputs:
    print(output.text)
```



Sampling parameters:

![sample](/assets/images/sampling.png)

