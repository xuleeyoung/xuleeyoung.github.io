---
title: Decision Trees
date: 2025-03-06
categories: [Tech, Machine Learning]
tags: [model, basics]     # TAG names should always be lowercase
---

### Decision Tree

决策树（Decision Tree）是一种用于分类和回归的树形结构模型，它通过一系列的 **条件判断（if-else 规则）** 来对数据进行划分，最终达到分类或预测的目的。决策树模型的核心思想是 **递归地将数据集划分成多个子集，直到每个子集中的数据点尽可能“纯净”**（即属于同一类别或具有相似的数值。

决策树主要分为两类：

1. **分类决策树（Classification Tree，CART 分类）**：用于处理分类任务，目标变量是离散的（例如“是/否”，“猫/狗”等。
2. **回归决策树（Regression Tree，CART 回归）**：用于处理回归任务，目标变量是连续的（例如房价预测。

常见的决策树算法：

- **ID3（Iterative Dichotomiser 3）**：基于 **信息增益** 选择划分属性。
- **C4.5**：ID3的改进版，使用 **信息增益率** 选择划分属性，并支持缺失值处理。
- **CART（Classification and Regression Trees）**：基于 **基尼系数** 选择划分属性，同时支持分类和回归。

#### ID3

用熵衡量数据纯度，信息增益计算如下：

![entropy](/assets/images/entropy.png)

![info-gain](/assets/images/information_gain.png)

ID3倾向于选择取值较多的特征，导致过拟合

#### C4.5

ID3的改进版本，采用信息增益率作为划分标准

![c45](/assets/images/c45.png)

#### CART(Classification and Regression Tree)

是一种二叉决策树，每次划分只能产生两个子节点，可同时处理分类和回归问题。遍历每个特征的每个划分点：

![cart](/assets/images/cart.png)

预测时，回归树输出的是叶子结点所有样本的均值，分类树输出的是叶子结点中所有样本出现最多的类型。



Decision tree常见超参数：

- `max_depth`: 最大深度，防止过拟合
- `min_samples_split`: 控制一个节点继续分裂的最少样本数
- `min_samples_leaf`: 叶子结点的最少样本树
- `max_leaf_node`: 最多叶子总数
- `criterion`: 分裂标准，分类任务有基尼系数、信息增益；回归任务有均方误差、绝对误差
- `splitter`: 分裂策略，可选择最佳分裂策略，也可选择随机策略(随机森林)
- `max_features`: 每次分裂可选的最大特征数，提高树的多样性，随机森林
- 剪枝相关



### Bagging & Boosting

- Bagging: 各个弱学习器并行运算，final classifier is a (majority) vote of each weak learner, (for regression, final ensemble output is the average of each outputs), Bagging可以减少variance， 但不能减小bias。如Random Forest
- Boosting: 训练一个加法模型(几个弱学习器串联，最终预测结果为弱学习器加权和)， 每一步学习一个弱学习器，并放大错误样本权重，以使得下一个弱学习器关注错误样本。如Adaboost



### Boosting Tree

![boosting](/assets/images/boosting.png)

![boosting1](/assets/images/boosting1.png)



### GBDT

Boosting Tree (残差) + CART树

![gbdt](/assets/images/gbdt.png)

对于回归问题MSE损失，负梯度即等于残差

对于分类问题交叉熵损失，负梯度也等于残差



### XGBoost

对MSE loss函数进行二阶泰勒展开

![xg1](/assets/images/xgboost2.png)

![xg2](/assets/images/xgboost3.png)

![xg3](/assets/images/xgboost.png)

其中$w_j$ 为第$j$ 个叶子结点的预测分数，即落到该叶子结点的所有样本的预测值。

XGBoost的打分函数，每次分裂节点时需计算分裂前后的分数增益：

![xg4](/assets/images/xgboost1.png)







