---
title: BERT
date: 2025-01-29
categories: [Tech, Machine Learning]
tags: [model, transformer, bert]     # TAG names should always be lowercase
---

A PyTorch implementation of Google's 2018 [BERT](https://arxiv.org/pdf/1810.04805)

### Model Architecture

![bert](/assets/images/bert.png)

1. BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation. BERT_BASE (L=12, H=768, A=12, Total Parameters=110M) and BERT_LARGE (L=24, H=1024, A=16, Total Parameters=340M).

2. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.
   - WHY GPT(decoder-only) better? 
   
     Decoder架构预训练目标与推理时的目标一致，模型能够在预训练阶段学习到更接近推理所需的知识；自回归模型目标更符合文本生成任务，而BERT更适合文本理解/总结；next word prediction等价于最大化整个sequence的probability，符合自监督模型概率。

### Pre-training

#### Task 1 Masked LM

### **Why Does BERT Introduce Noise into Masked Language Model (MLM) Training?**

The **issue** with BERT's pretraining is that the `[MASK]` token **never appears** in real-world fine-tuning or inference. This creates a **pretraining-finetuning mismatch**, where the model **learns to rely too much on the presence of `[MASK]` tokens**, but during fine-tuning, those tokens are absent.

![bert_MLM](/assets/images/bert_MLM.png)



#### Task 2 Next Sentence Prediction (NSP)

Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).



#### Pretraining Data

BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words).

![bert_input](/assets/images/bert_input.png)



### Fine-tuning(SFT)

1. Question Answering (SQuAD)
2. Text Classification
3. Masked Language Modeling

![finetune](/assets/images/bert_finetune.png)

#### Tokenization

可以使用 **Hugging Face Transformers** 库中的 BERT Tokenizer 进行 WordPiece Tokenization

```python
from transformers import DistilBertTokenizer, DistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

![bpe](/assets/images/BPE.png)

![wordpiece](/assets/images/wordpiece.png)

**WordPiece**: WordPiece **基于统计频率** 进行词汇构造，**核心思想**：

- 从 **字符级** 词典开始 (`['a', 'b', ..., 'z']`)，然后 **不断合并高频子词**
- 目标：**构建一个固定大小的 vocabulary**（BERT 选取 **30,522** 个 token）

当我们使用 **WordPiece Tokenizer** 处理输入文本时，它会执行 **贪心匹配（Greedy Longest-Match-First Algorithm）**：

1. **优先匹配最长的子词**
2. **如果整个单词不在词汇表中，则拆分为更小的子词**
3. **子词（subword）在非首词时加上前缀 `"##"`，表示其依附于前面的 token**

**BPE**: 

```python
from collections import Counter

def train_bpe(corpus, num_merges=10):
    """
    训练 BPE（Byte Pair Encoding）模型
    :param corpus: 输入文本列表
    :param num_merges: 进行多少次合并操作
    :return: 词汇表
    """
    # 统计初始词频，每个词拆成字符（模拟空格分词）
    words = [" ".join(word) + " </w>" for word in corpus]  # 末尾加特殊结束符 "</w>"
    vocab = Counter(words)

    # 统计合并操作
    merges = {}

    for i in range(num_merges):
        pairs = Counter()
        
        # 统计所有单词中的相邻字符对
        for word, freq in vocab.items():
            symbols = word.split()
            for j in range(len(symbols) - 1):
                pairs[(symbols[j], symbols[j+1])] += freq
        
        # 选择最频繁的字符对合并
        if not pairs:
            break
        best_pair = max(pairs, key=pairs.get)
        merges[best_pair] = i  # 记录合并顺序
        
        # 重新构建词汇表
        new_vocab = {}
        for word, freq in vocab.items():
            new_word = " ".join(word.split()).replace(" ".join(best_pair), "".join(best_pair))
            new_vocab[new_word] = freq
        vocab = new_vocab
    
    return merges

def apply_bpe(word, merges):
    """
    使用 BPE 规则对单词进行分词
    """
    word = " ".join(word) + " </w>"
    symbols = word.split()
    
    for merge, _ in sorted(merges.items(), key=lambda x: x[1]):
        if merge[0] in symbols and merge[1] in symbols:
            i = 0
            while i < len(symbols) - 1:
                if symbols[i] == merge[0] and symbols[i+1] == merge[1]:
                    symbols = symbols[:i] + ["".join(merge)] + symbols[i+2:]
                else:
                    i += 1
    return symbols

# 示例
corpus = ["hello", "hell", "help", "hero", "helicopter"]
merges = train_bpe(corpus, num_merges=10)

print("BPE 词汇表合并规则:", merges)

# 分词示例
word = "helicopter"
tokens = apply_bpe(word, merges)
print("BPE 分词结果:", tokens)

```

**BBPE**：

对于英文、拉美体系的语言来说使用BPE分词足以在可接受的词表大小下解决OOV的问题，但面对中文、日文等语言时，其稀有的字符可能会不必要的占用词汇表，因此考虑使用字节级别byte-level解决不同语言进行分词时OOV的问题。具体的，BBPE考虑将一段文本的UTF-8编码(UTF-8保证任何语言都可以通用)中的一个字节256位不同的编码作为词表的初始化基础Subword。

相比ASCII只能覆盖英文中字符，UTF-8编码创建的本身就是为了通用的将世界上不同的语言字符尽可能全部用一套编码进行编号，同时相比UTF-32对于每个字符都采用4位字节（byte）过于冗长。改进的UTF-8编码是一个变长的编码，有1～4个范围的字节(bytes)长度。对于不同语言中字符采用不同长度的字节编码，例如英文字符基本都是1个字节（byte），中文汉字通常需要2～3个字节。



### PyTorch Implementation

```python
# Embedding
class BERTEmbedding(nn.Module):
    """
    BERT Embedding which is consisted with under features
        1. TokenEmbedding : normal embedding matrix
        2. PositionalEmbedding : adding positional information using sin, cos
        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)

        sum of all these features are output of BERTEmbedding
    """

    def __init__(self, vocab_size, embed_size, dropout=0.1):
        """
        :param vocab_size: total vocab size
        :param embed_size: embedding size of token embedding
        :param dropout: dropout rate
        """
        super().__init__()
        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)
        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)
        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)
        self.dropout = nn.Dropout(p=dropout)
        self.embed_size = embed_size

    def forward(self, sequence, segment_label):
        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)
        return self.dropout(x)
      
     
    
# BERT: multilayer Transformer Encoder
class BERT(nn.Module):
    """
    BERT model : Bidirectional Encoder Representations from Transformers.
    """

    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):
        """
        :param vocab_size: vocab_size of total words
        :param hidden: BERT model hidden size
        :param n_layers: numbers of Transformer blocks(layers)
        :param attn_heads: number of attention heads
        :param dropout: dropout rate
        """

        super().__init__()
        self.hidden = hidden
        self.n_layers = n_layers
        self.attn_heads = attn_heads

        # paper noted they used 4*hidden_size for ff_network_hidden_size
        self.feed_forward_hidden = hidden * 4

        # embedding for BERT, sum of positional, segment, token embeddings
        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)

        # multi-layers transformer blocks, deep network
        self.transformer_blocks = nn.ModuleList(
            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])

    def forward(self, x, segment_info):
        # attention masking for padded token
        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)
        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)

        # embedding the indexed sequence to sequence of vectors
        x = self.embedding(x, segment_info)

        # running over multiple transformer blocks
        for transformer in self.transformer_blocks:
            x = transformer.forward(x, mask)

        return x
      
      
      
# Pre-training Tasks: NSP + Masked LM
class BERTLM(nn.Module):
    """
    BERT Language Model
    Next Sentence Prediction Model + Masked Language Model
    """

    def __init__(self, bert: BERT, vocab_size):
        """
        :param bert: BERT model which should be trained
        :param vocab_size: total vocab size for masked_lm
        """

        super().__init__()
        self.bert = bert
        self.next_sentence = NextSentencePrediction(self.bert.hidden)
        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)

    def forward(self, x, segment_label):
        x = self.bert(x, segment_label)
        return self.next_sentence(x), self.mask_lm(x)


class NextSentencePrediction(nn.Module):
    """
    2-class classification model : is_next, is_not_next
    """

    def __init__(self, hidden):
        """
        :param hidden: BERT model output size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, 2)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x[:, 0]))


class MaskedLanguageModel(nn.Module):
    """
    predicting origin token from masked input sequence
    n-class classification problem, n-class = vocab_size
    """

    def __init__(self, hidden, vocab_size):
        """
        :param hidden: output size of BERT model
        :param vocab_size: total vocab size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, vocab_size)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x))
```

