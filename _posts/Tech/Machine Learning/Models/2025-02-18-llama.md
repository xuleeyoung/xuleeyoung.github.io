---
title: Llama
date: 2025-02-18
categories: [Tech, Machine Learning]
tags: [model, transformer, llama, nlp]     # TAG names should always be lowercase
---

A PyTorch implementation of Google's 2018 [BERT](https://arxiv.org/pdf/1810.04805)

### Model Architecture

![bert](/assets/images/bert.png)

1. BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the original implementation. BERT_BASE (L=12, H=768, A=12, Total Parameters=110M) and BERT_LARGE (L=24, H=1024, A=16, Total Parameters=340M).
2. Critically, however, the BERT Transformer uses bidirectional self-attention, while the GPT Transformer uses constrained self-attention where every token can only attend to context to its left.
   - WHY GPT(decoder-only) better? 

### Pre-training

#### Task 1 Masked LM

### **Why Does BERT Introduce Noise into Masked Language Model (MLM) Training?**

The **issue** with BERT's pretraining is that the `[MASK]` token **never appears** in real-world fine-tuning or inference. This creates a **pretraining-finetuning mismatch**, where the model **learns to rely too much on the presence of `[MASK]` tokens**, but during fine-tuning, those tokens are absent.

![bert_MLM](/assets/images/bert_MLM.png)



#### Task 2 Next Sentence Prediction (NSP)

Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).



#### Pretraining Data

BooksCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2,500M words).

![bert_input](/assets/images/bert_input.png)



### Fine-tuning(SFT)

1. Question Answering (SQuAD)
2. Text Classification
3. Masked Language Modeling

![finetune](/assets/images/bert_finetune.png)

#### Tokenization

可以使用 **Hugging Face Transformers** 库中的 BERT Tokenizer 进行 WordPiece Tokenization

```python
from transformers import DistilBertTokenizer, DistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

text = "Replace me by any text you'd like."
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
```

**WordPiece**: WordPiece **基于统计频率** 进行词汇构造，**核心思想**：

- 从 **字符级** 词典开始 (`['a', 'b', ..., 'z']`)，然后 **不断合并高频子词**
- 目标：**构建一个固定大小的 vocabulary**（BERT 选取 **30,522** 个 token）

当我们使用 **WordPiece Tokenizer** 处理输入文本时，它会执行 **贪心匹配（Greedy Longest-Match-First Algorithm）**：

1. **优先匹配最长的子词**
2. **如果整个单词不在词汇表中，则拆分为更小的子词**
3. **子词（subword）在非首词时加上前缀 `"##"`，表示其依附于前面的 token**

**BPE**: 



### PyTorch Implementation

```python
# Embedding
class BERTEmbedding(nn.Module):
    """
    BERT Embedding which is consisted with under features
        1. TokenEmbedding : normal embedding matrix
        2. PositionalEmbedding : adding positional information using sin, cos
        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)

        sum of all these features are output of BERTEmbedding
    """

    def __init__(self, vocab_size, embed_size, dropout=0.1):
        """
        :param vocab_size: total vocab size
        :param embed_size: embedding size of token embedding
        :param dropout: dropout rate
        """
        super().__init__()
        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)
        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)
        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)
        self.dropout = nn.Dropout(p=dropout)
        self.embed_size = embed_size

    def forward(self, sequence, segment_label):
        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)
        return self.dropout(x)
      
     
    
# BERT: multilayer Transformer Encoder
class BERT(nn.Module):
    """
    BERT model : Bidirectional Encoder Representations from Transformers.
    """

    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):
        """
        :param vocab_size: vocab_size of total words
        :param hidden: BERT model hidden size
        :param n_layers: numbers of Transformer blocks(layers)
        :param attn_heads: number of attention heads
        :param dropout: dropout rate
        """

        super().__init__()
        self.hidden = hidden
        self.n_layers = n_layers
        self.attn_heads = attn_heads

        # paper noted they used 4*hidden_size for ff_network_hidden_size
        self.feed_forward_hidden = hidden * 4

        # embedding for BERT, sum of positional, segment, token embeddings
        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)

        # multi-layers transformer blocks, deep network
        self.transformer_blocks = nn.ModuleList(
            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])

    def forward(self, x, segment_info):
        # attention masking for padded token
        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)
        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)

        # embedding the indexed sequence to sequence of vectors
        x = self.embedding(x, segment_info)

        # running over multiple transformer blocks
        for transformer in self.transformer_blocks:
            x = transformer.forward(x, mask)

        return x
      
      
      
# Pre-training Tasks: NSP + Masked LM
class BERTLM(nn.Module):
    """
    BERT Language Model
    Next Sentence Prediction Model + Masked Language Model
    """

    def __init__(self, bert: BERT, vocab_size):
        """
        :param bert: BERT model which should be trained
        :param vocab_size: total vocab size for masked_lm
        """

        super().__init__()
        self.bert = bert
        self.next_sentence = NextSentencePrediction(self.bert.hidden)
        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)

    def forward(self, x, segment_label):
        x = self.bert(x, segment_label)
        return self.next_sentence(x), self.mask_lm(x)


class NextSentencePrediction(nn.Module):
    """
    2-class classification model : is_next, is_not_next
    """

    def __init__(self, hidden):
        """
        :param hidden: BERT model output size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, 2)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x[:, 0]))


class MaskedLanguageModel(nn.Module):
    """
    predicting origin token from masked input sequence
    n-class classification problem, n-class = vocab_size
    """

    def __init__(self, hidden, vocab_size):
        """
        :param hidden: output size of BERT model
        :param vocab_size: total vocab size
        """
        super().__init__()
        self.linear = nn.Linear(hidden, vocab_size)
        self.softmax = nn.LogSoftmax(dim=-1)

    def forward(self, x):
        return self.softmax(self.linear(x))
```

