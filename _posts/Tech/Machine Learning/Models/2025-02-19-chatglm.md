---
title: ChatGLM
date: 2025-02-19
categories: [Tech, Machine Learning]
tags: [model, transformer, nlp]     # TAG names should always be lowercase
---

ChatGLM

### Background

主流的预训练框架主要有三种：

1. **autoregressive自回归模型（AR模型）**：代表作GPT。本质上是一个left-to-right的语言模型。**通常用于生成式任务**，在长文本生成方面取得了巨大的成功，比如自然语言生成（NLG）领域的任务：摘要、翻译或抽象问答。当扩展到十亿级别参数时，表现出了少样本学习能力。缺点是单向注意力机制，在NLU任务中，无法完全捕捉上下文的依赖关系。
2. **autoencoding自编码模型（AE模型）**：代表作BERT。是**通过某个降噪目标（比如MLM）训练的双向文本编码器**。编码器会产出适用于NLU任务的上下文表示，但无法直接用于文本生成。
3. **encoder-decoder（Seq2seq模型）**：代表作T5。采用双向注意力机制，**通常用于条件生成任务**，比如文本摘要、机器翻译等。

三种预训练框架各有利弊，没有一种框架在以下三种领域的表现最佳：自然语言理解（NLU）、无条件生成以及条件生成。T5曾经尝试使用MTL的方式统一上述框架，然而自编码和自回归目标天然存在差异，简单的融合自然无法继承各个框架的优点。

在这个天下三分的僵持局面下，GLM诞生了。

**GLM模型基于autoregressive blank infilling方法，结合了上述三种预训练模型的思想**。

### Architecture

GLM在原始single Transformer的基础上进行了一些修改：

1. 重组了LN和残差连接的顺序；
2. 使用单个线性层对输出token进行预测；
3. 激活函数从ReLU换成了GeLU。

![glm](/assets/images/glm.png)

### Pre-training

1. **自编码思想**：在输入文本中，随机删除连续的tokens。
2. **自回归思想**：顺序重建连续tokens。在使用自回归方式预测缺失tokens时，模型既可以访问corrupted文本，又可以访问之前已经被预测的spans。
3. **span shuffling + 二维位置编码技术**。
4. 通过改变缺失spans的数量和长度，自回归空格填充目标可以为条件生成以及无条件生成任务预训练语言模型。

上述方法适合于NLU任务。作者希望可以训练一个既可以解决NLU任务，又具备文本生成能力的模型。因此除了空格填充目标之外，还需要增加一个生成长文本目标的任务。具体包含以下两个目标：

1. **文档级别**。从文档中采样一个文本片段进行mask，且片段长度为文档长度的50%～100%。这个目标用于长文本生成。
2. **句子级别**。限制被mask的片段必须是完整句子。多个片段需覆盖原始tokens的15%。这个目标是用于预测完整句子或者段落的seq2seq任务。

### Fine-tuning

![glm-ft](/assets/images/glm-ft.png)

对于下游NLU任务来说，通常会将预训练模型产出的序列或tokens表达作为输入，使用线性分类器预测label。所以预训练与微调之间存在天然不一致。

作者按照PET的方式，将下游NLU任务重新表述为空白填充的生成任务。具体来说，比如给定一个已标注样本(x, y)，将输入的文本x转换成一个包含mask token的完形填空问题。比如，情感分类任务可以表述为："{SENTENCE}. It’s really [MASK]"。输出label y也同样会被映射到完形填空的答案中。“positive” 和 “negative” 对应的标签就是“good” 和 “bad。



### ChatGLM 2

1. **使用了RoPE替换二维位置编码**。这也是GLM中提出的亮点设计之一。但是目前大部分主流的LLMs都在使用RoPE，所以大势所趋。当前版本仍然采用了最初的RoPE设计，事实上现在的RoPE经过了xPOS→线性内插→NTK-Aware Scaled RoPE→…若干次进化。
2. **Multi-Query Attention**：这是一种共享机制的Attention，相比Multi-Head Attention，其Query部分没有区别，Key和Value可以只用一个Head。计算时，对Key和Value进行expand或者repeat操作，使它们填充到与Query一样的维度，后续计算就与Multi-Head Attention没区别。
3. **Attention Mask**: V1的attention mask分了2部分，Part A和Part B，Part A部分是双向Attention（代码中的[prefix_attention_mask](https://huggingface.co/THUDM/chatglm-6b/blob/main/modeling_chatglm.py#L963)），Part B部分是Causal Attention(原代码文件中的get_masks函数)。在V2版本，全部换成了Causal Attention，不再区分是Part A还是Part B，**完全变成了decoder-only的架构**。
4. **多目标任务**：Chat版本主要还是用的gMask生成式任务，但是在V1版本的代码还能看到mask、gMask等字样，V2已经摒弃了这些特殊token，原因与Attention Mask一致，均因为变成了decoder-only的架构，不再需要区分Part A和Part B。



![glm-v](/assets/images/glm-v.png)
