---
title: Vision Transformer
date: 2025-01-28
categories: [Tech, Machine Learning]
tags: [model, transformer]     # TAG names should always be lowercase
---

A PyTorch implementation for Vision Transformer(ViT) introduced by [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)

### Model Architecture

![vit](/assets/images/vit.png)

1. Prepended learnable embedding, similar to BERT's [CLS] token, whose state at the output of the Transformer encoder serves as image representation.

2. Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings.

3. **Inductive Bias** (归纳偏置) 归纳偏置是算法和模型的“先验知识”或“默认倾向”，它决定了：

   1. 算法如何对新数据进行预测。
   2. 算法在有限训练数据上的泛化能力。

   这种偏置是算法本身的一部分，并且是不可避免的，因为没有偏置的情况下，学习算法无法从有限的数据中归纳出有用的模式。这也体现了著名的 **“No Free Lunch Theorem”（无免费午餐定理）**：没有一个单一的学习算法可以在所有问题上表现良好。如卷积神经网络（CNN）：

   - 假设数据具有局部性和空间不变性（如图像中边缘和特征是局部的）。
   - 偏置：通过卷积核捕获局部特征。

   ViT: We note that Vision Transformer has **much less image-specific inductive bias** than CNNs. In CNNs, **locality**, two-dimensional neighborhood structure, and **translation equivariance** are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are **global**. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.

### PyTorch Implementation

```python
class ViT(nn.Module):
    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):
        super().__init__()
        image_height, image_width = pair(image_size)
        patch_height, patch_width = pair(patch_size)

        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'

        num_patches = (image_height // patch_height) * (image_width // patch_width)
        patch_dim = channels * patch_height * patch_width
        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
        
        # Patch Embedding
        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            nn.LayerNorm(dim),
        )

        # Positional Embedding  
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        
        # Prepended learnable class token
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        
        self.dropout = nn.Dropout(emb_dropout)

        self.transformer = Transformer(depth, heads, dim, block_size=num_patches)

        self.pool = pool
        self.to_latent = nn.Identity()

        self.mlp_head = nn.Linear(dim, num_classes)

    def forward(self, img):
        x = self.to_patch_embedding(img)
        b, n, _ = x.shape

        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)
        x = torch.cat((cls_tokens, x), dim=1)
        x += self.pos_embedding[:, :(n + 1)]
        x = self.dropout(x)

        x = self.transformer(x)

        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]

        x = self.to_latent(x)
        return self.mlp_head(x)
```

### Experiment

![exp](/assets/images/vit_exp.png)

**Why Does ViT Perform Better with Larger Pretraining Datasets?**

1. ViT Lacks Inductive Bias (Compared to CNNs)

   CNNs: Built-in Inductive Bias (Translation Equivariance & Locality)

- **Locality**: Filters process small patches, **forcing the model to focus on local details**.
- **Translation Equivariance**: Features remain **stable regardless of object position**.

- **Advantage**: Even with small datasets, CNNs generalize well.

​		ViT **does not** assume locality or spatial hierarchies.

- It learns **global dependencies** via **self-attention**, requiring **more diverse data to generalize well**.
- **Without large data, ViT may overfit small patterns rather than learning robust features.**

2. ViT Learns Long-Range Dependencies Better with More Data

   CNNs: Fixed Local Receptive Field

- CNN filters **only capture local features** and build global features **hierarchically**.
- **Problem:** Limited ability to model **long-range dependencies** unless **very deep**.

​		ViT: Global Self-Attention

- ViT uses **self-attention** that **directly connects all image patches**.
- **Problem:** Small datasets → model overfits short-range patterns.
- **Solution:** **More data helps ViT generalize long-range dependencies.**





位置编码插值：

当预训练权重中的位置编码最大token seq长度小于当前模型最大输入seq长度时，需要用bi-cubic插值解决长度超出部分无法位置编码的问题

```python
def interpolate_pos_embed(pos_embed_checkpoint, visual_encoder):        
    # interpolate position embedding
    embedding_size = pos_embed_checkpoint.shape[-1]
    num_patches = visual_encoder.patch_embed.num_patches
    num_extra_tokens = visual_encoder.pos_embed.shape[-2] - num_patches
    # height (== width) for the checkpoint position embedding
    orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)
    # height (== width) for the new position embedding
    new_size = int(num_patches ** 0.5)

    if orig_size!=new_size:
        # class_token and dist_token are kept unchanged
        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]
        # only the position tokens are interpolated
        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]
        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)
        pos_tokens = torch.nn.functional.interpolate(
            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)
        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)
        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)
        print('reshape position embedding from %d to %d'%(orig_size ** 2,new_size ** 2))
        
        return new_pos_embed    
    else:
        return pos_embed_checkpoint
```









