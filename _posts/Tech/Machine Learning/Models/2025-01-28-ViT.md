---
title: Vision Transformer
date: 2025-01-28
categories: [Tech, Machine Learning]
tags: [model, transformer]     # TAG names should always be lowercase
---

A PyTorch implementation for Vision Transformer(ViT) introduced by [AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE](https://arxiv.org/pdf/2010.11929)

### Model Architecture

![vit](/assets/images/vit.png)

1. Prepended learnable embedding, similar to BERT's [CLS] token, whose state at the output of the Transformer encoder serves as image representation.

2. Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings.

3. **Inductive Bias** (归纳偏置) 归纳偏置是算法和模型的“先验知识”或“默认倾向”，它决定了：

   1. 算法如何对新数据进行预测。
   2. 算法在有限训练数据上的泛化能力。

   这种偏置是算法本身的一部分，并且是不可避免的，因为没有偏置的情况下，学习算法无法从有限的数据中归纳出有用的模式。这也体现了著名的 **“No Free Lunch Theorem”（无免费午餐定理）**：没有一个单一的学习算法可以在所有问题上表现良好。如卷积神经网络（CNN）：

   - 假设数据具有局部性和空间不变性（如图像中边缘和特征是局部的）。
   - 偏置：通过卷积核捕获局部特征。

   ViT: We note that Vision Transformer has **much less image-specific inductive bias** than CNNs. In CNNs, **locality**, two-dimensional neighborhood structure, and **translation equivariance** are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are **global**. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.

### PyTorch Implementation

```python
class ViT(nn.Module):
    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):
        super().__init__()
        image_height, image_width = pair(image_size)
        patch_height, patch_width = pair(patch_size)

        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'

        num_patches = (image_height // patch_height) * (image_width // patch_width)
        patch_dim = channels * patch_height * patch_width
        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'
        
        # Patch Embedding
        self.to_patch_embedding = nn.Sequential(
            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
            nn.LayerNorm(patch_dim),
            nn.Linear(patch_dim, dim),
            nn.LayerNorm(dim),
        )

        # Positional Embedding  
        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))
        
        # Prepended learnable class token
        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
        
        self.dropout = nn.Dropout(emb_dropout)

        self.transformer = Transformer(depth, heads, dim, block_size=num_patches)

        self.pool = pool
        self.to_latent = nn.Identity()

        self.mlp_head = nn.Linear(dim, num_classes)

    def forward(self, img):
        x = self.to_patch_embedding(img)
        b, n, _ = x.shape

        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)
        x = torch.cat((cls_tokens, x), dim=1)
        x += self.pos_embedding[:, :(n + 1)]
        x = self.dropout(x)

        x = self.transformer(x)

        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]

        x = self.to_latent(x)
        return self.mlp_head(x)
```

