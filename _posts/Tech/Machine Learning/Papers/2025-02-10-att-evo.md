---
title: Attention Evolution
date: 2025-02-05
categories: [Tech, Machine Learning, Papers]
tags: [machine learning, nlp, transformers]     # TAG names should always be lowercase
math: true
---

### MHA: multi-head attetion

Transformer原始注意力机制

BERT, GPT2, Llama, Qwen

```python
class MHA(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.num_heads = config.num_heads
        self.hidden_size = config.hidden_size
        self.scale = self.hidden_size ** -0.5

        self.qkv = nn.Linear(self.hidden_size, 3 * self.hidden_size)
        self.att_drop = nn.Dropout(0.1)
        self.fc = nn.Linear(self.hidden_size, self.hidden_size)
        self.proj_drop = nn.Dropout(0.1)
        

    def forward(
        self,
        x,
        mask=None,
    ):
        B, T, C = x.shape
        qkv = self.qkv(x).view(B, T, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn + mask
        attn = nn.Softmax(attn, dim=-1)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2).view(x.shape)
        x = self.fc(x)
        x = self.proj_drop(x)
        
        return x

```



### MQA: multi-query attetion

For fast decoder inference, use multiple query heads but single key/value heads.

Uptraining : convert the checkpoint(mean pooling multiple projection matrices innto single projection matrices) and additional pertaining to allow the model to adapt to its new structure.

PaLM, Gemini

### GQA: Grouped-query attetion

![gqa](/assets/images/QGA.png)

虽然 MQA 中完成的计算量与 MHA 相同，但从内存读取的数据量（键、值）只是以前的一小部分。当受内存带宽限制时，这可以实现更好的计算利用率。它还减少了内存中 KV 缓存的大小，为更大的批量大小留出了空间。

key头的减少会带来潜在的准确性下降。此外，需要在推理时利用这种优化的模型需要在启用 MQA 的情况下进行训练（或至少使用大约 5% 的训练量进行微调）。**MQA 和 GQA 等优化通过减少存储的key头和value头的数量来帮助减少 KV 缓存所需的内存**。

MQA会导致quality degradation和training instability

GQA采用MHA和MQA的折中方法

Llama 2,3, Qwen2

```python
class GQA(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.q_heads = config.q_heads
        self.kv_heads = config.kv_heads
        self.hidden_size = config.hidden_size
        self.scale = self.hidden_size ** -0.5

        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size)
        self.kv_embed_dim = self.hidden_size // q_heads * kv_heads
        self.k_proj = nn.Linear(self.hidden_size, self.kv_embed_dim)
        self.v_proj = nn.Linear(self.hidden_size, self.kv_embed_dim)
        
        self.att_drop = nn.Dropout(0.1)
        self.fc = nn.Linear(self.hidden_size, self.hidden_size)
        self.proj_drop = nn.Dropout(0.1)
        

    def forward(
        self,
        x,
        mask=None,
    ):
        B, T, C = x.shape
        q, k, v = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        
        q = q.view(B, T, self.q_heads, C // self.q_heads).transpose(1, 2)
        k = k.view(B, T, self.kv_heads, self.kv_embed_dim // self.kv_heads).transpose(1, 2)
        v = v.view(B, T, self.kv_heads, self.kv_embed_dim // self.kv_heads).transpose(1, 2)
        
        num_head_groups = self.q_heads // self.kv_heads
        q = q.view(B, num_head_groups, self.kv_heads, T, C // self.q_heads)
        
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn + mask
        attn = nn.Softmax(attn, dim=-1)
        attn = self.attn_drop(attn)
        x = (attn @ v).view(B, self.q_heads, T, C // self.q_heads).transpose(1, 2).view(x.shape)
        x = self.proj(x)
        x = self.proj_drop(x)
        
        return x
```



### MLA: Multi-head latent attention

![mla](/assets/images/MLA.png)

DeepSeek

对K, V的projection matrix低秩分解，将K, V投影到远小于hidden_size的维度，减少了KV cache的内存。



