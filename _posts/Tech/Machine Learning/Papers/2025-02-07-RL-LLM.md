---
title: Reinforcement Learning in LLM
date: 2025-02-05
categories: [Tech, Machine Learning, Papers]
tags: [machine learning, nlp, transformers]     # TAG names should always be lowercase
math: true
---

### RLHF (Reinforcement Learning from Human Feedback)

两次对模型的微调：GPT3模型 → SFT模型 → RL模型，其实这里始终都是同一个模型，只是不同过程中名称不同。

- **需要SFT模型的原因**： GPT3模型不一定能够保证根据人的指示、有帮助的、安全的生成答案需要人工标注数据进行微调。
- **需要RM模型的原因**：标注排序的判别式标注成本远远低于生成答案的生成式标注。
- **需要RL模型的原因**：在对SFT模型进行微调时生成的答案分布也会发生变化，会导致RM模型的评分会有偏差，需要用到强化学习.

SFT是将模型调整到符合对话场景，遵循人类指令，或者某个垂直领域

RHLF使模型输出更符合人类偏好，更安全更有用的信息。

