---
title: Rotary Positional Embedding
date: 2025-02-05
categories: [Tech, Machine Learning, Papers]
tags: [machine learning, nlp, transformers]     # TAG names should always be lowercase
math: true
---

#### Low-rank Adaptation

LoRA is a parameter-efficient fine-tuning method, which freezes the pre-trained model weigts and injects trainable rank decomposition matrices into each layer of the Transformer architecture.

![lora](/assets/images/lora.png)

The learned over-parameterized models in fact reside on a low intrinsic dimension. 

$$W = W_0 + BA$$

Random Aussian initialization for $A$, and zero for $B$.

#### Implementation

##### loralib [repo](https://github.com/microsoft/LoRA/tree/main)

```python
# ===== Before =====
# layer = nn.Linear(in_features, out_features)

# ===== After ======
import loralib as lora
# Add a pair of low-rank adaptation matrices with rank r=16
layer = lora.Linear(in_features, out_features, r=16)

import loralib as lora
model = BigModel()
# This sets requires_grad to False for all parameters without the string "lora_" in their names
lora.mark_only_lora_as_trainable(model)
# Training loop
for batch in dataloader:
   ...
    
# ===== Before =====
# torch.save(model.state_dict(), checkpoint_path)
# ===== After =====
torch.save(lora.lora_state_dict(model), checkpoint_path)


# Load the pretrained checkpoint first
model.load_state_dict(torch.load('ckpt_pretrained.pt'), strict=False)
# Then load the LoRA checkpoint
model.load_state_dict(torch.load('ckpt_lora.pt'), strict=False)
```

loralib is the primary implementation of LoRA. It provides lora layers for all kinds of neural networks (nn.Linear,  nn.Embedding, nn.Conv2d). More flexible to change the original model structure.

##### Huggingface PEFT

```python
```





