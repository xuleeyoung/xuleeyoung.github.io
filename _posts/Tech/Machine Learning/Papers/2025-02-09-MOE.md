---
title: Mixture of Experts
date: 2025-02-05
categories: [Tech, Machine Learning, Papers]
tags: [machine learning, nlp, transformers]     # TAG names should always be lowercase
math: true
---

### Background

MoE，全称为Mixed Expert Models，翻译过来就是混合专家模型。

**模型规模是提升模型性能的关键因素之一**，这也是为什么今天的大模型能取得成功。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。

MoE 的一个显著优势是它们**能够在远少于 Dense 模型所需的计算资源下进行有效的预训练**。这意味着在相同的计算预算条件下，可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。

### Architecture

Two main elements(NNs):

1. Sparse MoE layer: 包含若干个experts(例如8个)，替代传统的dense FFN
2. Gate network/Router: 路由网络决定每个token传到哪个专家FFN

比较传统Dense model和MOE：

```python
# Llama-7B
embedding
     word_embeddings
         weight torch.Size([32000, 4096])
encoder
     layers.0.input_norm.weight
         torch.Size([4096])
     layers.0.self_attention.query_key_value.weight
         torch.Size([12288, 4096])
     layers.0.self_attention.dense.weight
         torch.Size([4096, 4096])
     layers.0.post_attention_norm.weight
         torch.Size([4096])
     layers.0.mlp.dense_h_to_4h.weight
         torch.Size([22016, 4096])
     layers.0.mlp.dense_4h_to_h.weight
         torch.Size([4096, 11008])
     ...
     final_norm.weight
         torch.Size([4096])
output_layer
     weight
         torch.Size([32000, 4096])

# Llama-7B MOE 
embedding
     word_embeddings
         weight torch.Size([32000, 4096])
encoder
     layers.0.input_norm.weight
         torch.Size([4096])
     layers.0.self_attention.query_key_value.weight
         torch.Size([12288, 4096])
     layers.0.self_attention.dense.weight
         torch.Size([4096, 4096])
     layers.0.post_attention_norm.weight
         torch.Size([4096])
     layers.0.mlp.router.weight
         torch.Size([2, 4096])
     layers.0.mlp.router.bias
         torch.Size([2])
     layers.0.mlp.local_experts.0.dense_h_to_4h.weight
         torch.Size([22016, 4096])
     layers.0.mlp.local_experts.1.dense_h_to_4h.weight
         torch.Size([22016, 4096])
     layers.0.mlp.local_experts.0.dense_4h_to_h.weight
         torch.Size([4096, 11008])
     layers.0.mlp.local_experts.1.dense_4h_to_h.weight
         torch.Size([4096, 11008])
     ...
     final_norm.weight
         torch.Size([4096])
output_layer
     weight
         torch.Size([32000, 4096])
```

MOE参数量扩大了10倍，但是训练和推理速度远快于具有相同参数量的稠密模型。Transformer模型scaling law表面增加参数量可以提高性能，在资源有限情况下，MOE架构通过解捆绑参数量和计算量，高效实现scaling。

MOE缺陷：推理时对内存要求很高，需要将所有experts参数加载进VRAM，同时微调也成本更高，且容易泛化不足，引发过拟合。



以下介绍采用MOE架构的LLM：

### Switch Transformer

![switch_transformer](/assets/images/switch_transformer.png)

1. 采用简化的单专家路由策略，路由网络只选择top-1的专家，减少了路由计算，减少了每个专家的expert capacity, 从而减少通信成本
2. expert capacity = (# of tokens per batch) / (# of experts) * capacity factor, 理想状态是tokens平均分配给每个expert(每个expert并行处理相同数量的tokens)，这样可以避免计算资源浪费。capacity factor > 1: 意味着为每个专家提供额外缓冲空间，使其能够处理超出平均分配数量的token。容量因子过高会导致计算资源浪费，switch transformer在低容量因子情况下表现更佳。
3. 负载均衡损失：MOE中专家往往分布在不同device上，理想情况下每个专家处理相同数量的数据，以实现资源平均利用，实际数据分布不均匀，导致某些专家过载，某些专家空闲，辅助损失函数促进专家之间负载均衡。

```python
import torch

def load_balancing_loss(gate_logits, expert_mask):
    """
    计算负载均衡损失

    参数:
    gate_logits: shape (batch_size, num_experts), 门控网络的 softmax 输出
    expert_mask: shape (batch_size, num_experts), 0/1 掩码，表示 Token 被分配到哪个专家

    返回:
    loss: 负载均衡损失值
    """

    # 计算 p_i：每个专家被门控网络分配的概率均值
    p_i = gate_logits.mean(dim=0)  # 按 batch 维度求均值

    # 计算 f_i：每个专家实际处理的 Token 数量占比
    f_i = expert_mask.float().mean(dim=0)  # 按 batch 维度求均值

    # 计算负载均衡损失
    loss = torch.sum(f_i * p_i) * expert_mask.shape[1]  # num_experts 作为系数
    return loss

```



#### Switch Transformer Implementation

```python
class SwitchTransformersTop1Router(nn.Module):
    """
    Router using tokens choose top-1 experts assignment.

    This router uses the same mechanism as in Switch Transformer (https://arxiv.org/abs/2101.03961) and V-MoE
    (https://arxiv.org/abs/2106.05974): tokens choose their top experts. Items are sorted by router_probs and then
    routed to their choice of expert until the expert's expert_capacity is reached. **There is no guarantee that each
    token is processed by an expert**, or that each expert receives at least one token.

    """
    def __init__(self, config: SwitchTransformersConfig):
        super().__init__()
        self.num_experts = config.num_experts
        self.expert_capacity = config.expert_capacity
        self.classifier = nn.Linear(config.hidden_size, self.num_experts, bias=config.router_bias)

    def forward(self, hidden_states: torch.Tensor) -> Tuple:
        r"""
        Generic forward function for every Router class. Each Router expects to have the same input hidden states
        (`hidden_states`) corresponding to the hidden states for each token, the `expert_capacity` corresponding to the
        number of tokens the Router will send to each expert, some Routers can send up to few tokens to each expert.

        Each Router works as the following: it expects the hidden states for each token, gets the `router_probs` and
        `router_logits` from the `router_weights`. This will assign for each token, the raw probability to be assigned
        to an expert. Then each Router class will have to define its own `_compute_routing_instructions`.

        Args:
            hidden_states (`torch.Tensor`) :
                [num_groups, tokens_per_group, hidden_dim] inputs to send to experts.
        Returns:
            Tuple[`torch.Tensor`, `torch.Tensor`, `torch.Tensor`] Tuple containing the expert index, the router probs
            and the router logits. The router probabilities and logits are required to compute the loss.
        """
        router_logits = self.classifier(hidden_states)
        router_probs = nn.functional.softmax(router_logits, dim=-1, dtype=self.dtype)

        expert_index = torch.argmax(router_probs, dim=-1)
        expert_index = torch.nn.functional.one_hot(expert_index, num_classes=self.num_experts)

        # Mask tokens outside expert capacity. Sum over each sequence
        token_priority = torch.cumsum(expert_index, dim=-2)
        # mask if the token routed to the expert will overflow
        expert_capacity_mask = token_priority <= self.expert_capacity
        expert_index = expert_index * expert_capacity_mask

        router_probs = torch.max(router_probs, dim=-1).values.unsqueeze(-1)
        return expert_index, router_probs, router_logits
      

class SwitchTransformersSparseMLP(nn.Module):
    r"""
    Implementation of the Switch Transformers Sparse MLP module.
    """

    def __init__(self, config: SwitchTransformersConfig, expert_class: nn.Module = SwitchTransformersDenseActDense):
        super().__init__()
        # Step 1: Get the correct router according to its class
        self.router = SwitchTransformersTop1Router(config)

        # Step 2: Get the experts
        self.experts = nn.ModuleDict()
        for idx in range(config.num_experts):
            self.experts[f"expert_{idx}"] = expert_class(config)

    def forward(self, hidden_states):
        # Step 1: Get the router_mask from the router as wel as the probabilities
        router_mask, router_probs, router_logits = self.router(hidden_states)
        expert_index = torch.argmax(router_mask, dim=-1)

        # The routers introduced might not always map all the tokens, to a router, which means that some hidden states
        # can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the seleced ones.

        next_states = hidden_states.clone()

        router_mask = router_mask.bool()
        batch_size, seq_len, num_experts = router_mask.shape
        idx_mask = router_mask.reshape(batch_size * seq_len, num_experts).sum(dim=0)
        idx_mask = torch.nonzero(idx_mask, as_tuple=True)[
            0
        ].tolist()  # length: number of "activated" expert / value: index
        for idx in idx_mask:
            next_states[router_mask[:, :, idx]] = getattr(self.experts, "expert_{}".format(idx))(
                hidden_states[router_mask[:, :, idx]]
            )

        hidden_states = router_probs * next_states
        return hidden_states, (router_logits, expert_index)


class SwitchTransformersLayerFF(nn.Module):
    r"""
    Switch Transformers Feed Forward layer module. This is a wrapper around the Mixture of Experts module.

    Parameters:
        config : ([`SwitchTransformersConfig`]): Model configuration class with all the parameters of the model.
            Initializing with a config file does not load the weights associated with the model, only the
            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
        is_sparse (`bool`):
            Whether the MLP layer is a `Sparse` layer (contains a Mixture of Experts) or not
    """

    def __init__(self, config: SwitchTransformersConfig, is_sparse=False):
        super().__init__()
        self.is_sparse = is_sparse

        # Check if it is a sparse layer, if not then it is a dense layer
        if not self.is_sparse:
            self.mlp = SwitchTransformersDenseActDense(config)
        else:
            self.mlp = SwitchTransformersSparseMLP(config)

        self.layer_norm = SwitchTransformersLayerNorm(config.d_model, eps=config.layer_norm_epsilon)
        self.dropout = nn.Dropout(config.dropout_rate)

    def forward(self, hidden_states, output_router_logits):
        forwarded_states = self.layer_norm(hidden_states)
        forwarded_states = self.mlp(forwarded_states)

        if isinstance(forwarded_states, tuple):
            forwarded_states, router_tuple = forwarded_states
        else:
            router_tuple = None

        output = hidden_states + self.dropout(forwarded_states)

        if output_router_logits and router_tuple is not None:
            output = (output, router_tuple)

        return output
```

### Mixtral-8x7B

Mixtral-8x7B is a decoder-only Transformer with the following architectural choices:

- Mixtral is a Mixture of Experts (MoE) model with 8 experts per MLP, with a total of 45 billion parameters. To learn more about mixture-of-experts, refer to the [blog post](https://huggingface.co/blog/moe).
- Despite the model having 45 billion parameters, the compute required for a single forward pass is the same as that of a 14 billion parameter model. This is because even though each of the experts have to be loaded in RAM (70B like ram requirement) each token from the hidden states are dispatched twice (top 2 routing) and thus the compute (the operation required at each forward computation) is just 2 X sequence_length.

### Deepseek

除了GPT-4和Switch Transformer，国内的团队DeepSeek 也**开源了国内首个 MoE** 大模型 **DeepSeekMoE**。

- DeepSeekMoE **2B可接近2B Dense，仅用了17.5%计算量。**
- DeepSeekMoE **16B性能比肩 LLaMA2 7B 的同时，仅用了40%计算量。**
- DeepSeekMoE **145B 优于Google 的MoE大模型GShard，而且仅用 28.5%计算量即可匹配 67B Dense 模型的性能。**

