---
title: RAG
date: 2025-01-25
categories: [Tech, Machine Learning, Papers]
tags: [machine learning, nlp, transformers]     # TAG names should always be lowercase
math: true
---

### Retrieval-Augmented Generation

**Retrieval-Augmented Generation (RAG)** is a **hybrid NLP approach** that combines:

1. **Retrieval-based methods** → Fetch relevant documents from an external knowledge source (e.g., Wikipedia, databases, vector stores). _Non-parametric memory_
2. **Generative models** → Use a **Transformer-based model** (e.g., GPT, BART, T5) to generate responses based on retrieved documents. _Parametric memory_

why RAG?

1. LLM hallucination: LLM 预训练时缺少实时和外部信息导致模型生成看起来合理但是实际错误的信息
2. Pre-trained LLM 在面对最新知识和复杂任务时表现不足

RAG通过检索外部知识库获得额外语料，并通过in-context learning来改进LLM生成效果





什么是大模型幻觉？如何缓解？

大模型生成内容是虚假的、不符合事实或逻辑的，但是在语法上是合理的。

原因：

1. 大模型训练目标本质是auto-regressive的，最大化下一个token的分布概率，它是基于上下文内容去推测下一个最有可能的单词，大模型本身并不会“感受到”事实或逻辑；训练模型的目标是生产连贯的文本，并不能保证符合事实逻辑
2. 训练数据过时，或本身逻辑错误

解决方法：

1. 使用高质量的符合事实或逻辑的文本SFT，强化学习，通过reward增强模型推理能力
2. CoT
3. RAG



#### Architecture

![rag](/assets/images/rag.png)

##### Retriver: DPR

DPR consists of **two BERT-based encoders**:

1. **Query Encoder** → Converts queries into dense vectors.
2. **Context Encoder** → Converts documents (passages) into dense vectors.

Find the most relevant passgae by computing dot-product similarity. Training uses contrastive loss over (query, positive passage, negative passage) triplets.



##### Generator: BART

A pre-trained encoder-decoder seq2seq transformer, combining BERT and GPT architectures.





#### RAG Engine

RAGFlow





RAG vs. Fine-tuning:

如果模型库要在特定的领域工作，假设这个模型在这个领域能力很强，那么只需要在面对全新内容（如私有内容，公司内部文档）时搭配RAG即可

如果领域能力还需提高，那就需要尝试微调

