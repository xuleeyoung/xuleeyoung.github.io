---
title: RAG
date: 2025-01-25
categories: [Tech, Machine Learning, Papers]
tags: [machine learning, nlp, transformers]     # TAG names should always be lowercase
math: true
---

### Retrieval-Augmented Generation

**Retrieval-Augmented Generation (RAG)** is a **hybrid NLP approach** that combines:

1. **Retrieval-based methods** → Fetch relevant documents from an external knowledge source (e.g., Wikipedia, databases, vector stores). _Non-parametric memory_
2. **Generative models** → Use a **Transformer-based model** (e.g., GPT, BART, T5) to generate responses based on retrieved documents. _Parametric memory_

why RAG?

1. LLM hallucination: LLM 预训练时缺少实时和外部信息导致模型生成看起来合理但是实际错误的信息
2. Pre-trained LLM 在面对最新知识和复杂任务时表现不足

RAG通过检索外部知识库获得额外语料，并通过in-context learning来改进LLM生成效果

#### Architecture

![rag](/assets/images/rag.png)

##### Retriver: DPR

DPR consists of **two BERT-based encoders**:

1. **Query Encoder** → Converts queries into dense vectors.
2. **Context Encoder** → Converts documents (passages) into dense vectors.

Find the most relevant passgae by computing dot-product similarity. Training uses contrastive loss over (query, positive passage, negative passage) triplets.



##### Generator: BART

A pre-trained encoder-decoder seq2seq transformer, combining BERT and GPT architectures.





#### RAG Engine

RAGFlow





RAG vs. Fine-tuning:

如果模型库要在特定的领域工作，假设这个模型在这个领域能力很强，那么只需要在面对全新内容（如私有内容，公司内部文档）时搭配RAG即可

如果领域能力还需提高，那就需要尝试微调

