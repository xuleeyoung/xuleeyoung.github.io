---
title: Parameter Efficient Tuning For E2E NLG Challenge
date: 2025-01-25
categories: [Tech, Projects]
tags: [bert, machine learning, NLP]     # TAG names should always be lowercase
math: true
---

### E2E NLG Challenge

The **E2E dataset** is a structured **text generation dataset** designed for **end-to-end data-to-text generation**. It is widely used in **natural language generation (NLG)** tasks where models must generate **natural language descriptions** from structured data (like tables or key-value pairs).

ðŸ“Œ **Full Name**: **E2E NLG Challenge Dataset**

ðŸ“Œ **Purpose**: Train models to generate **natural language text** from **structured meaning representations (MRs)**.

ðŸ“Œ **Domain**: **Restaurant recommendations**

ðŸ“Œ **Type**: **Data-to-text generation**

ðŸ“Œ **Size**: ~50,000 instances

ðŸ“Œ **Task**: Convert structured **meaning representations (MRs)** into **fluent sentences**.

### Parameter Efficient Tuning

We utilizes three main PEFT method for LLM Pre-trained Models:

1. Prompt Tuning
2. Prefix Tuning
3. P-tuning

Project codes: [repo](https://github.com/xuleeyoung/Parameter-Efficient-Tuning-For-E2E-NLG-Challenge)



Some problems encountered during implementation:

- Paddings in tokenizer: if `padding` is not enabled in tokenizer, the returned tensor of each input text sequence will have different length. This is not efficient for training on GPU (cannot batch tensors on GPU). The trainer may throw an shape mismatch error. 

> Solution: Turn on `padding=True` to match the longest sequence dynamically. You can also set `padding="max_length"` and specify `max_length=N` to pad to a fixed length.
{: .prompt-tip }

- Remember to set labels=-100 at the positions where the loss calculation should be ignored. (e.g. \[PAD\] token).

> According to transformers docs for GPT2LMHeadModel:
>
> ```python
> labels (torch.LongTensor of shape (batch_size, sequence_length), optional) â€” Labels for language modeling. Note that the labels are shifted inside the model, i.e. you can set labels = input_ids Indices are selected in [-100, 0, ..., config.vocab_size] All labels set to -100 are ignored (masked), the loss is only computed for labels in [0, ..., config.vocab_size]
> ```
{: .prompt-tip }

- GPT-2 tokenizer does not have \[PAD\] token

- Source codes for loss calculation in transformers: logits will be shifted

  ```python
  shift_logits = logits[..., :-1, :].contiguous()
  shift_labels = labels[..., 1:].contiguous()
  # Flatten the tokens
  loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
  loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
  ```

- Trainer configs:

  ```python
  # Basic
  training_args = TrainingArguments(
      output_dir="./results",         # Directory to save model checkpoints
      overwrite_output_dir=True,      # Overwrite previous checkpoint if exists
      do_train=True,                  # Enable training
      do_eval=True,                   # Enable evaluation
      evaluation_strategy="epoch",    # Evaluate at the end of each epoch
      save_strategy="epoch",          # Save model at the end of each epoch
  )
  
  # Training Hyperparams
  training_args = TrainingArguments(
      per_device_train_batch_size=8,  # Training batch size per GPU
      per_device_eval_batch_size=8,   # Evaluation batch size per GPU
      gradient_accumulation_steps=2,  # Accumulate gradients over multiple steps
      num_train_epochs=3,             # Number of epochs
      learning_rate=5e-5,             # Learning rate (AdamW optimizer)
      weight_decay=0.01,              # Apply L2 weight decay
      lr_scheduler_type="linear",     # Learning rate schedule (e.g., cosine, constant)
      warmup_steps=500,               # Number of warmup steps before full LR
  )
  
  # Logging
  training_args = TrainingArguments(
      logging_dir="./logs",           # Log directory for TensorBoard
      logging_strategy="steps",       # Log every few steps
      logging_steps=50,               # Log frequency
      log_level="info",               # Logging level (info, warning, debug)
      report_to=["tensorboard"],      # Where to log (TensorBoard, WandB, etc.)
  )
  
  
  # Checkpoints
  training_args = TrainingArguments(
      save_strategy="steps",          # Save checkpoint every `save_steps`
      save_steps=1000,                # Save model every 1000 steps
      save_total_limit=2,             # Only keep the last 2 checkpoints
      load_best_model_at_end=True,    # Load best model based on `metric_for_best_model`
      metric_for_best_model="loss",   # Save the model with the lowest loss
      greater_is_better=False,        # Loss should be minimized
  )
  
  # Distributed / Multi-GPU
  training_args = TrainingArguments(
      fp16=True,                      # Enable mixed precision (faster training)
      fp16_opt_level="O1",            # Mixed precision optimization level
      optim="adamw_torch",            # Use PyTorch's AdamW optimizer
      dataloader_num_workers=4,       # Number of workers for data loading
      ddp_find_unused_parameters=False, # Needed for multi-GPU (DDP)
  )
  ```

> Tips: If you want to let Trainer automatically save checkpoints, your model should have `save_pretrained()` method or inherits from `PreTrainedModel`.
{: .prompt-tip }

- Token Generation: 

  ```python
  for _ in range(required_chars):
    outputs = model(**inputs, labels=inputs['input_ids'])
    logits = outputs.logits[0][-1,:]
    probs = nn.functional.softmax(logits, dim=-1)
    _, next_chr = torch.topk(probs, k=1, dim=-1)
    inputs['input_ids'] = torch.cat((inputs['input_ids'], next_chr.unsqueeze(0)), dim=-1)
    inputs['attention_mask'] = torch.cat((inputs['attention_mask'], torch.tensor([[1]]).to('cuda')), dim=-1)
      
    output_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)
  ```

  
